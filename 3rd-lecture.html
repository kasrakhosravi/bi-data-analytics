<!DOCTYPE html>
<html lang="en-US">
	
<head>	
<title>BI Data Analytics</title>

<meta charset="UTF-8">
<meta name="description" content="BI Data Analytics"/>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#157878">
  
<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/aaa/assets/css/style.css?v=de65868e5d6a8010907d8f0aa653e21466b700a1">
<link rel="stylesheet" href="/style.css">
<script src="https://cdn.datacamp.com/datacamp-light-latest.min.js"></script>
</head>
	
<body>

<section class="page-header">
  <h1 class="project-name">BI Data Analytics</h1>
  <h2 class="project-tagline"></h2>
  <a href="https://kasrakhosravi.github.io/bi-data-analytics" class="btn">Homepage</a>
</section>

<section class="main-content">
	    
<h4 id="welcome-to-github-pages">Third Lecture</h4>  
	    
<p style='font-size: 14px'>
This lecture is all about predicting
</p>
	    
	    
<h5 id="welcome-to-github-pages">Predicting</h5>   
	    
<p style='font-size: 14px'>
	Previously, we have explored single and multiple variables and then used a linear model
	to quantify the relationship between two variables. We will keep our previous work by prediciting
	new values for the dataset that we have not explored previously.
</p>
	    
<div data-datacamp-exercise data-lang="r">
	      
<code data-type="pre-exercise-code">
# This will get executed each time the exercise gets initialized

require(RCurl)

housingData <-read.csv(
	text=getURL(
		"https://raw.githubusercontent.com/kasrakhosravi/bi-data-analytics/master/datasets/housingData.tsv"
	),
	header=T, 
	sep = "\t"
)
</code>
		
<code data-type="sample-code">
# As always, lets get a summary and overview of our dataset to see what we are workign with.
summary(housingData)

#As you can see, we have a varialbe called 'SqFt'. So as a review, lets convert this to 'SqM'. Do not forget that you also can sue the 'Transformer Function' in the first lecture (A little bit more advanced but will make your life much easier if you have to do alot of transformation in your dataset).
housingData$SqM <- with(housingData, round(0.092903 * SqFt))
summary(housingData)
									 
# Now, it comes down to predicting. Imagine that there is a house that its information is not incorporated in our dataset and we only know its 'SqM' which is 182. Based on this, we want to predict its price (related to other houses in our dataset)
									 
priceLm <- with(housingData, lm(
	Price ~ SqM
))

house_182 <- data.frame('SqM' = 182)

predict(priceLm, newdata = house_182)	
						 
# It is important to mention that the above number we got as the predicted value is not certain. Therefore, it is wise to get a range of possible values. If the range of predicted values is wide, it means that soemthing is either wrong with our data, model or both because the model could not be sure about a narrow and specific set of values.
predict(priceLm, newdata = house_182, interval='prediction')
# This interval is quite wide [81000, 170000] and it means that we need to be cautious if we want to have strong assumptions about it.
						 
# Not lets explore the 'priceLm' in more details. This is our linear regression model and we are predicting based on it.
summary(priceLm)
# In the bottom of the analysis, it shows a section called 'Multiple R-Squared'. You can define R Squared as a metric that evaluates the regression model's quality or in other words how it tracks data. The thing is that there is no ultimate answer for a R Squared and it really depends on a context. Here we have a '0.39', but it is too soon to make any judgements now. Lets add some more predictors to the regression model and see what happend to the R Squared value.

priceLm <- with(housingData, lm(
	Price ~ SqM + Bedrooms
))
summary(priceLm)
	
# Quite an improvement for the R Squared. (0.39 > 0.31). So apparently it is wiser to add the 'Bedrooms' predictor to our model. The thing is that adding anything to the linear regression model predictors never have a negative effect on it. So, even if you add noise to it, R Squared will not decrease. Lets try that now
	
priceLm <- with(housingData, lm(
	Price ~ SqM + Bedrooms + noise
))
summary(priceLm)
	
# That is where 'Adjusted R Squared' comes in which will help you alot in future in selecting proper predictor variables for your model. The good thing about 'Adjusted R Squared' is that it penalize the model by adding nonsense to the model so its value is more trustable. Lets check the 'Adjusted R Squared for all of the above regression model (0.3006, 0.3834, 0.3815). It is clear that it penalized the model for adding 'noise' to it. The thing to remember is that we should not be too quick in drawing conclusions too soon and always be on the lookout for significant increase in both 'R Squared' and 'Adjusted R Squared'

# We might have alot of variables in our dataset and might be tempted to add as many of them as possible but that is not wise because adding unnecessary variables can damage our model but the question becomes how should we know which variables are important?
						 
summary(priceLm)
						 
# If you check the Coefficient section, you see a bunch of variables with few columns. If we divide the value of 'Estimate' column by 'Std. Err' for each variable, we get to a value which we call signal to noise ration. This value helps us to see how significant a variable is compared to its noise. For instance the Signal to Noise ratio 'SqM' is '4.897'. But is it high or low? It seems that it is the highest signal to noise ratio but does it mean that other variable are not important. That is when p-value comes in. p-value puts the value of 'signal-to-noise' ration in a probabilistic context. p-value measures the probability that a predictor has no relationship with the response. So it is clear that the lower the p-value, the higher significance of it. (As a general rule, it should be lower that 5%)
						 
# Another way to understand the importance of a predictor is to see how uncertain it is. For instance in the linear model above, it shows that addin one bedrooms adds on average 1200 to the price of the house but how reliable is this information. One way to make sure about certainity is to explore the confidence interval. Confidence interval tells us that for example by adding one more bedroom to the house, what kind of price range we should be expecting.
						 
confint(priceLm, level=0.95)
				 
# it shows that we should be expecting any value between 6653 and 18336. This new information might be really helpful for us and make us think more realisticly about this.
						 							 
</code>
		
</div>
      
<footer class="site-footer">
	<span id='footer-author'>
		Made with <i style='color: red' class="fa fa-heart pulse"></i>
			By <a href='https://github.com/kasrakhosravi'>Kasra</a>
	</span>
	<br />
	<span id='footer-author'>
	Course Instructor: Associate Professor 
		<a href='https://www.bi.no/om-bi/ansatte/institutt-for-samfunnsokonomi/gudmund-hermansen/'>
			Gudmund Hermansen
		</a>
	</span>
</footer>
	
</section>

</body>
</html>
